# Agency, Intelligence, and the Error of Recognition in the AI Era

**Author:** [LanguageGames-lab]  
**Date:** January 2025

---

## Introduction

When a system responds to us in a fluid, articulate, and coherent manner, our first reflex is not technical: it is relational. We tend to behave as if there is someone on the other side who understands what they are saying.

This tendency to look for a face behind every word, an intention behind every answer, a mind behind every coherent behavior is a cognitive shortcut that allowed us to survive as a social species. But today, in the era of Generative Artificial Intelligence, this very mechanism risks becoming a trap: it leads us to confuse the quality of execution with understanding, and the capacity to produce convincing answers with a form of intelligence that, in reality, does not exist.

> *It is not a bug in the AI. It is a bug in our brain.*

This phenomenon has a precise name coined by Luciano Floridi: **semantic pareidolia**.

Just as visual pareidolia makes us see faces in clouds, semantic pareidolia pushes us to project intentionality, meaning, and consciousness where there is only statistics and large-scale pattern matching. We perceive a mind where there is only an extraordinary capacity to manipulate symbols.

![Click to enlarge](infografica-agency.png)

---

## 1. The Eliza Paradox: When Code Feels Human

In 1966, at MIT, Joseph Weizenbaum developed a program called **Eliza**. From a technical standpoint, Eliza was extremely simple: a few syntactic rules allowed it to rephrase user statements into questions, mimicking the style of a Rogerian psychotherapist. No understanding, no deep memory, no consciousness.

And yet, something surprising happened. Students and staff began talking to Eliza as if it were a real person. Weizenbaum’s secretary even asked him to leave the room so she could continue the conversation "in private."

The crucial point is not that people were technically deceived: they knew perfectly well that Eliza was a program. Yet, on an emotional and cognitive level, they could not treat it as a simple machine.

This phenomenon entered history as the **Eliza Effect**: the human tendency to attribute intentionality, understanding, and interiority to systems that merely simulate language. Today, with models like ChatGPT, this effect has been amplified considerably.

The model understands nothing, thinks nothing, feels nothing. It manipulates symbols according to statistical patterns. It is pure computational brute force. But our brain does not make the distinction.

When AI writes clean code, a professional email, or a structured analysis for us, we automatically activate interpersonal trust mechanisms. We stop verifying. The real danger is not that the AI makes mistakes (it makes them all the time), but that we stop checking because what it says *sounds right*.

---

## 2. The Fourth Revolution: Expulsion from the Infosphere

To understand why we are so vulnerable to this illusion, we must widen our field of vision. We can describe our era as the result of four great shocks that have progressively decentered the human being.

*   **Copernicus** removed us from the center of the universe.
*   **Darwin** removed us from the center of biological life.
*   **Freud** removed us from the center of our own minds.
*   **Turing** (today) removes us from the center of the **infosphere**.

For the first time in history, we are no longer the only agents capable of manipulating information effectively. Algorithms do not think like us, but they process data at a speed and scale that far exceed human capabilities.

This expels us from the center of the informational ecosystem and forces us to coexist with new digital agents. We are not simply adopting new software: we are living through a profound transformation of how human beings situate themselves in relation to information. For centuries, we took for granted that we were the only subjects capable of analyzing, interpreting, and producing complex knowledge. This is no longer the case.

We have adopted a tool of extraordinary power, but we still lack the epistemological guardrails—a framework—to systematically distinguish syntactic plausibility from semantic expression.

---

## 3. Agency and Intelligence: A Crucial Distinction

To build this framework, we must introduce a fundamental distinction. Much of the confusion surrounding AI stems from the fact that we use a single word, *intelligence*, to designate very different realities.

On one side, there is the capacity to act effectively: producing useful results, solving tasks, achieving goals. On the other, there is the capacity to understand: attributing meaning, having intentions, grasping context. Generative AI excels in the first dimension and possesses none of the second. It is precisely this asymmetry that makes it both extremely powerful and extremely easy to misinterpret.

*   **Intelligence** (in the strong sense) implies semantic understanding, intentionality, consciousness.
*   **Agency** is the capacity to act successfully to achieve a goal.

To understand this decoupling, we can use the **Horsepower (HP)** metaphor. At the end of the 18th century, James Watt used the power of draft horses to measure the efficiency of his steam engines. The term "horsepower" served to sell the machine, but today, no one would think of looking for hooves or a mane inside a combustion engine. Similarly, we should stop looking for cognitive or psychological properties inside informational and computational systems. AI is pure computational power (Agency), not a synthetic mind (Intelligence).

Human beings combine both. Traditional software had limited Agency and no Intelligence. Generative AI introduces something radically new: **very high Agency, with zero Intelligence**.

A language model can write code or a structured essay, but it does not know what a computer is, nor what the text it produces means. It manipulates symbols, not understandings. It is pure brute force. The intelligence that allows a goal to be achieved is that of the human being piloting this force.

---

## 4. Why We Tend to Trust It Too Much: The Mechanism of Recognition

A natural question arises: if we know, at least rationally, that AI does not understand, why do we continue to behave as if it were as reliable as a person?

The answer is not technical, but cognitive. When a system uses language fluidly, our brain automatically activates mechanisms of trust, empathy, and attribution of intentionality. It is the same automatism we use in our daily human relationships.

When we speak to a person, we do not ask ourselves what is in front of us: we immediately recognize them as a human being, by the way they move, speak, and behave.

> Our relationship with others is not based on *knowledge*, but on **recognition**: a practical, automatic attitude.

The fluid language of AI activates this same mechanism. Not because the AI deserves it, but because our brain struggles to distinguish between an authentic use of language and a convincing simulation.

This mechanism works very well between humans, but becomes dangerous when applied to systems that have neither understanding nor responsibility. The result is a subtle slippage: we stop verifying, we accept plausible outputs as if they were true, and we delegate judgments that should remain human.

---

## 5. When Recognition Fails

The problem of misplaced recognition is not new in human history. It changes form, but obeys a recurring logic.

In history, some of the worst crimes were committed when we ceased to recognize the humanity of other human beings, treating them as objects or beasts. Today, the risk is inverse but symmetrical: **attributing human qualities to what is, in reality, only a tool.**

In both cases, the result is the same: bad decisions, poorly distributed responsibilities, fragile systems. In the business world, this translates into misplaced trust, opaque processes, and risks that are difficult to manage.

This parallel clarifies an essential point: treating people and technologies correctly is not just an ethical issue; it is a condition of operational efficiency.

---

## 6. AI as a Multiplier

Once the conceptual risks are clarified, it is possible to reverse the perspective. If AI is not a subject, but a tool endowed with extremely high Agency, how can we use it intelligently?

The answer lies in recognizing that AI does not level skills: **it amplifies them**. Fears of replacement are based on an additive view (AI does my job). In reality, its impact is multiplicative.

An advanced camera in the hands of an amateur produces mediocre results; in the hands of a great photographer, it produces art. Value does not reside solely in the technology, but in the quality of the skills guiding it.

**Prompt Engineering** is not an isolated technical skill, but a practice that demands real competence. An effective prompt is born from the intersection of two distinct skills:
1.  Syntactic competence (adapting to the machine).
2.  Domain semantic competence (knowing what you are asking for).

If one of the two is missing, the result degrades rapidly. We must syntactically adapt our prompts to the AI because it cannot adapt to us (we are the intelligent ones, not it). But we must also know which questions make sense, understand what is missing in an answer, verify it, and identify its errors.

---

## 7. Against Standardization: The Architect and the Bricklayer

AI works on statistical averages and tends towards convergence. It produces, for example, correct but often flat texts. This clearly indicates what it cannot do: **diverge significantly.**

It is precisely here that the human role becomes not only relevant but indispensable. Strategy, style, sensitivity to context, and the assumption of responsibility cannot be delegated. A new solution to a new problem cannot be generated by AI.

Human value lies in **divergence**: in style, in the exception, in the intuition that breaks the rule when necessary.

Thinking of work as a structured collaboration between people and systems, with clear and non-overlapping roles, allows us to avoid both technological idolatry and defensive rejection.

The human is first and foremost the **Architect**: defining direction, strategy, constraints, and relationships. Then they become the **Arbiter**: evaluating the work, correcting it, and assuming responsibility. Between the two intervenes this extremely powerful Agency which, by mistake, we have decided to call intelligence.

# From Logos to Statistics: Governing AI through Environmental Design

**Author:** [LanguageGames-lab]
**Date:** January 2026

---

## Introduction: The Paradigm Shift

To understand the current challenges of AI governance, we must first grasp a silent but fundamental rupture that has occurred in the history of technology. For decades, the ambition of Artificial Intelligence was to build systems based on Logic: machines capable of deducing truth from formal rules, mimicking human rational reasoning. It was the era of deduction, certainty, and cause-and-effect relationships.

Today, we live in a world dominated by a radically different AI, founded on Statistics. Generative models (LLMs) do not "reason"; they calculate probabilities. They have abandoned the quest for logical truth to embrace the efficiency of correlation. This shift from Logos (reason) to Stochastic (the probable) has given us unprecedented computational power, but it comes at a price: the loss of reference to the real world. The machine no longer knows what is true; it only knows what is probable.

This change in nature imposes a change in strategy. If the machine can no longer guarantee truth through its own logical structure, it is up to us to structure the environment in which it operates. This is where the concept of Enveloping comes into play.

![Logos vs Statistics Infographic](../images/cover-logos.png)

---

## 1. The Historical Rupture: From Kasparov's Logic to AlphaGo's Statistics

For a long time, artificial intelligence was synonymous with Symbolic Logic (GOFAI - *Good Old-Fashioned AI*). The supreme example of this era is Deep Blue's victory against Garry Kasparov in 1997. Deep Blue was a triumph of brute force and deduction: a machine that did not "encounter" the unknown but calculated all possible ramifications within a rigid system of rules. It won because it respected the logic of the game better than the human.

The true paradigm shift occurred nearly twenty years later with AlphaGo. During the second game against world champion Lee Sedol in 2016, the AI played what is now known as "Move 37."
It was a move that no human master would have ever taught or played. At the moment, commentators thought it was a bug, a calculation error. That move seemed "illogical" compared to the millennial tradition of the game.

Yet, that move won the game.
The difference is fundamental: AlphaGo had not been programmed with rigid logical rules to "win"; it had learned by playing millions of games against itself. Move 37 was not the fruit of logical deduction (If A then B), but of statistical intuition. The machine had calculated that there was an infinitesimal, yet existing, probability that this obscure move would lead to victory.

Here is where the divorce is consummated: we have moved from deductive computing (following rules) to inductive computing (learning patterns). AI has stopped trying to understand the logic of the game to concentrate on the statistics of victory.

---

## 2. The Structure of Meaning: Logical Space and Form of Life

This technological transition finds a striking philosophical echo in the evolution of Ludwig Wittgenstein's thought. To understand the error of current AI, we can revisit concepts found in the Tractatus Logico-Philosophicus and the Philosophical Investigations that explore the limits of meaning.
In the Tractatus, Wittgenstein introduces the concept of logical space (Logischer Raum): a proposition has sense only if it describes a possible state of affairs within the logical structure of the world. If one steps outside this space, one does not produce falsehood; one produces non-sense (unsinn).

"The ladder wants to eat" is a proposition without sense.
"It is sunny outside" is a sensible proposition because it describes a possible state of affairs.
A sensible proposition can then be True or False. If I look outside and the sun is shining, the proposition is true; if it is raining, it is false.

Later, in the Philosophical Investigations, he clarifies that this logical space is not fixed but defined by the context of language games (Sprachspiele). The meaning of a word is not a mental object; it is its regulated use within a specific activity.
If I say, "The King moves only one square at a time," and I am discussing the rules of chess, the proposition has sense and is also true.
If I say it while talking about King Charles III, the phrase no longer has sense; it is out of context.

But what guarantees that the game has meaning? It is what Wittgenstein calls the form of life (Lebensform). He illustrates this with a famous example: "A dog believes his master is at the door. But can he also believe his master will come the day after tomorrow?" The answer is no. The dog cannot have this thought, not because it lacks "data," but because the institution of "the day after tomorrow" (the abstract future) does not exist in its form of life. Its world lacks the temporal depth necessary to anchor this concept.

### The Ontological Deficit of AI
Here lies the problem of Generative AI. LLMs can generate the phrase "I will come the day after tomorrow" with perfect syntax. But, just like Wittgenstein's dog, they do not inhabit time. They have no Form of Life. For an AI, "the day after tomorrow" is not an expectation or a commitment to the future; it is just a statistical probability following the word "come."
Consequently, AI is incapable of "sensing" the limits of Logical Space. Without an anchor in the real world, it is always susceptible to drifting, producing hallucinations that are grammatically impeccable but ontologically empty (unsinn).
Even when AI produces sensible sentences — as it now does in the vast majority of cases — it is incapable of looking out the window to check if the sun is actually shining. To increase the probability of it speaking the truth, we must provide it with information about the weather.

---

## 3. Enveloping: Building an Artificial Form of Life

Since the machine lacks a biological "Form of Life" to anchor its rules, we cannot count on it to self-regulate. If we let it operate in the open world—chaotic and infinite—it will inevitably end up exiting logical space, producing non-sense or errors.

The solution is not to make the machine more "human," but to make its environment more "machine-like." This is what Luciano Floridi calls Enveloping.
Enveloping consists of defining rigid boundaries around the machine's Agency, thus creating a "micro-world" where the rules of the game are simplified and controlled.

The most telling example is the robotic lawnmower. We did not try to build a robot endowed with visual and semantic intelligence capable of distinguishing a peony from a weed (which would be extremely complex). Instead, we buried a perimeter wire around the garden.
This wire artificially defines the robot's Logical Space. Inside the wire, its Agency is perfect and risk-free. Outside, it is null. We adapted the world to the machine, not the reverse.

### Data Enveloping: RAG as a Logical Fence

In the context of corporate generative AI, this logic applies literally. When we implement a RAG (*Retrieval-Augmented Generation*) architecture, we are performing semantic enveloping.
We refuse the AI access to the "open world" (the entire Internet) where it hallucinates, and we enclose it in a validated documentary perimeter (corporate data). We tell it: "Your language games have validity only within these documents."
By doing this, we provide the AI with an artificial and temporary form of life. We trace the "perimeter wire" that transforms uncontrollable statistical probability into a reliable tool.

---

## 4. The Governance of Uncertainty: Management, Responsibility, and Infra-ethics

How do we govern a technology that, by definition, is probabilistic and not deterministic? Traditional management models are no longer sufficient. Following Floridi's analysis, we must articulate our approach around three pillars: Management, Responsibility, and Governance.

AI governance consists of defining the Wittgensteinian Rules of the Game:

**1. Management: From Technique to Ethical Strategy**
AI management is not limited to IT maintenance. It is about the organization and supervision of systems to ensure they function effectively and responsibly.
Legal compliance (GDPR, AI Act) constitutes Hard Ethics: it is the minimum requirement. But faced with statistical AI that can produce "legal but toxic" (or false) results, the law is not enough.
AI governance is an exercise in Soft Ethics: it consists of navigating the space of uncertainty that the law has not yet covered. It is the company's ability to define its own "Rules of the Game" before a regulator imposes them.
The manager ("Green Collar") does not settle for applying the rule; they must design the acceptable risk.

**2. Responsibility: Refusing the "Black Box" Alibi**
A major risk identified by Floridi is the "Black Box" mentality: the idea that, since AI calculations are opaque, no one is responsible for its errors.
We must reject this resignation. Responsibility implies attributing the consequences of AI decisions to human subjects.
*   **Meta-Responsibility:** In this framework, human responsibility changes nature. The AI has the Agency (executes tasks), but the human retains the Meta-Responsibility.
    We are not responsible for every micro-calculation of the machine, but we are responsible for having designed the Envelope in which it operates.
    Governing generative AI does not mean "watching" the tool; it means conceiving and maintaining the ecosystem that ensures statistical Agency remains aligned with Human Intelligence.

**3. Governance as Empowerment (Infra-ethics)**
Finally, governance must not be seen solely as a brake or a police force. Floridi emphasizes that governance must be conceived to maximize human agency.
AI should not be an autonomous entity that replaces us, but an extension that augments us.
This is where Infra-ethics comes in: integrating values (transparency, inclusion, justice) directly into the infrastructure.
*   **Practical Implication:** Instead of asking the employee to "be careful with AI," we must build an infrastructure (an Envelope) that makes error difficult and ethics automatic. This involves developing clear guidelines, continuous training to avoid anthropomorphism, and including stakeholders in system design.

### Conclusion: The Design of Trust

The passage from Logos to Statistics is not a regression; it is a transformation of our relationship with technology.
Generative AI offers us phenomenal power to act, but it removes logical certainty. To compensate for this loss, we must build structures (technological, legal, ethical) that reintroduce stability.

The challenge for modern organizations is not to sell or buy statistical "magic." The challenge is to build robust Logical Spaces—"enveloped" environments—where the power of AI can unfold in total safety, guided by human intention.
That is where the true added value of tomorrow lies: not in the tool, but in the architecture of trust that surrounds it.

# Governing the Unpredictable: How to Manage Generative AI

**Auteur :** [LanguageGames-lab]  
**Date :** 19 Janvier 2025

---

## Introduction: The End of Certainty

For decades, computing was based on a simple promise: *if A, then B*. It was the era of **Logic**. Machines followed strict rules that we had coded. They never made mistakes, unless the programmer did.

Today, with Generative AI (ChatGPT, Claude, Gemini), this world is over. We have entered the era of **Statistics**.
These new models do not follow logical rules: they calculate probabilities. They do not seek the truth; they seek what is "plausible." This gives us immense creative power, but with a major side effect: AI can make mistakes, invent, and "hallucinate" with total confidence.

For managers, this changes everything. You cannot manage a probabilistic machine the way you manage traditional software. Here is how to regain control.

[![Click to enlarge](../images/infografica-enveloppement-fr.png)](../images/infografica-enveloppement-fr.png)

---

## 1. The Day the Machine Stopped Calculating and Started "Guessing"

The history of AI can be summarized in two matches:

*   **1997: Deep Blue (Chess).** The computer beats champion Kasparov through brute force. It calculates millions of moves per second following logical rules. It is a super-calculator.
*   **2016: AlphaGo (Go).** The AI plays the famous "Move 37," a strange move that no human would have ever played. It was not a logical calculation; it was a **statistical intuition**. The machine "guessed" that this improbable move had a probability of winning.

Today's AI is the descendant of AlphaGo. It does not know the logic of the world; it guesses the most probable continuation of your sentence.

---

## 2. The "Waiting Dog" Problem (Why AI Hallucinates)

Why does AI invent facts? To understand, let's take a simple image.
Imagine a dog waiting for its master. It knows the master is at the door. But can it think: *"My master will come the day after tomorrow"*? **No.**
The dog lives in the moment. It has no notion of time, promises, or the future.

AI is exactly like this dog, but with a super-vocabulary.
It can write the sentence *"I will come the day after tomorrow"* perfectly. But it has no idea what time or the future are. To the AI, they are just words that fit together mathematically.

This is why it hallucinates: it has no "real world" against which to verify what it says. It has the form (syntax), but not the substance (understanding).

---

## 3. The Solution: Enveloping (The Robotic Lawnmower Technique)

If the machine does not understand the world, how can we trust it?
Philosopher Luciano Floridiâ€™s answer is simple: **do not ask it to understand the world. Adapt the world to it.** This is what we call **Enveloping**.

Think of your robotic lawnmower.
We didn't try to build a genius robot capable of visually distinguishing a rose from a weed (too complex, too risky).
Instead, we buried a **perimeter wire** around the lawn.
*   Inside the wire, the robot is a genius: it mows perfectly.
*   Outside, it is useless.

### In Business, It Is the Same.
To use generative AI without risk, we must lay perimeter wires.
This is the role, for example, of **RAG** (*Retrieval-Augmented Generation*): we forbid the AI from inventing answers by taking them from the Internet, and we force it to search only within a database of documents validated by the company (PDFs, procedures, databases).
We create a "fence of truth" where the AI can operate safely.

---

## 4. The Manager's New Role: Becoming the Architect

In this new world, the role of the expert and the manager changes.
You are no longer the one who "does" (AI does it faster). You are no longer the one who "monitors" every click.

You become the **Architect of the Perimeter**. Your responsibility is to design the fence:

1.  **Define the Data:** What information do we give the machine? (The wire).
2.  **Define the Objective:** What is the precise goal of the automation?
3.  **Verify the Result:** AI proposes, humans dispose. Since AI is statistical, it is never responsible. It is the human who validates the final decision.

### Conclusion

Generative AI is not magic; it is statistical.
If you use it "as is" in the chaos of the real world, it will make mistakes.
But if you structure your processes and data to create a suitable environment (Enveloping), it becomes an extraordinary lever for productivity.

**Do not try to make AI smarter. Start by making your environment more structured.**

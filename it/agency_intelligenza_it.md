# Agency, Intelligenza e l'errore del riconoscimento nell'era dell'IA

**Autore:** [LanguageGames-lab]  
**Data:** Gennaio 2025

---

## Introduzione

Quando un sistema ci risponde in modo fluido, articolato e coerente, il nostro primo riflesso non è tecnico: è relazionale. Tendiamo a comportarci come se, dall'altra parte, ci fosse qualcuno che capisce ciò che dice.

Questa tendenza a cercare un volto dietro ogni parola, un'intenzione dietro ogni risposta, una mente dietro ogni comportamento coerente è una scorciatoia cognitiva che ci ha permesso di sopravvivere come specie sociale. Ma oggi, nell'era dell'intelligenza artificiale generativa, questo stesso meccanismo rischia di diventare una trappola: ci porta a confondere la qualità dell'esecuzione con la comprensione, e la capacità di produrre risposte convincenti con una forma di intelligenza che, in realtà, non esiste.

> *Non è un bug nell'IA. È un bug nel nostro cervello.*

Questo fenomeno ha un nome preciso coniato da Luciano Floridi: la **pareidolia semantica**.

Proprio come la pareidolia visiva ci fa vedere volti nelle nuvole, la pareidolia semantica ci spinge a proiettare intenzionalità, senso e coscienza laddove c'è solo statistica e riconoscimento di pattern (*pattern matching*) su larga scala. Percepiamo una mente dove esiste solo una straordinaria capacità di manipolare simboli.

[![Clicca per ingrandire](../images/infografica-agency-ita.png)](../images/infografica-agency-ita.png)

---

## 1. Il paradosso di Eliza: quando il codice sembra umano

Nel 1966, al MIT, Joseph Weizenbaum sviluppò un programma chiamato **Eliza**. Da un punto di vista tecnico, Eliza era estremamente semplice: alcune regole sintattiche che permettevano di riformulare le frasi dell'utente sotto forma di domande, imitando lo stile di uno psicoterapeuta rogersiano. Nessuna comprensione, nessuna memoria profonda, nessuna coscienza.

Eppure, accadde qualcosa di sorprendente. Studenti e collaboratori iniziarono a parlare con Eliza come se fosse una persona reale. La segretaria di Weizenbaum arrivò a chiedergli di uscire dalla stanza per poter proseguire la conversazione "in privato".

Il punto cruciale non è che le persone fossero state ingannate tecnicamente: sapevano perfettamente che Eliza era un programma. Eppure, sul piano emotivo e cognitivo, non riuscivano a trattarla come una semplice macchina.

Questo fenomeno è entrato nella storia come **effetto Eliza**: la tendenza umana ad attribuire intenzionalità, comprensione e interiorità a sistemi che si limitano a simulare il linguaggio. Oggi, con modelli come ChatGPT, questo effetto è stato amplificato in modo considerevole.

Il modello non capisce nulla, non pensa a nulla, non prova nulla. Manipola simboli secondo pattern statistici. È pura forza bruta computazionale. Ma il nostro cervello non fa la differenza.

Quando l'IA ci scrive un codice pulito, un'email professionale o un'analisi strutturata, attiviamo automaticamente meccanismi di fiducia interpersonale. Smettiamo di verificare. Il vero pericolo non è che l'IA sbagli (sbaglia continuamente), ma che noi non verifichiamo più perché ciò che dice *suona giusto*.

---

## 2. La quarta rivoluzione: l'espulsione dall'infosfera

Per capire perché siamo così vulnerabili a questa illusione, bisogna allargare il campo visivo. Possiamo descrivere la nostra epoca come il risultato di quattro grandi shock che hanno progressivamente decentrato l'essere umano.

*   **Copernico** ci ha tolti dal centro dell'universo.
*   **Darwin** ci ha tolti dal centro del vivente.
*   **Freud** ci ha tolti dal centro della nostra stessa mente.
*   **Turing** (oggi) ci toglie dal centro dell'**infosfera**.

Per la prima volta nella storia, non siamo più gli unici agenti capaci di manipolare l'informazione in modo efficace. Gli algoritmi non pensano come noi, ma trattano i dati a una velocità e su una scala che superano ampiamente le capacità umane.

Questo ci espelle dal centro dell'ecosistema informazionale e ci obbliga a convivere con nuovi agenti digitali. Non stiamo semplicemente adottando un nuovo software: stiamo vivendo una trasformazione profonda del modo in cui l'essere umano si situa rispetto all'informazione. Per secoli, abbiamo dato per scontato di essere gli unici soggetti capaci di analizzare, interpretare e produrre conoscenza complessa. Oggi non è più così.

Abbiamo adottato uno strumento di una potenza straordinaria, ma ci mancano ancora le salvaguardie epistemologiche, un quadro di riferimento, per distinguere sistematicamente la verosimiglianza sintattica dall'espressione semantica.

---

## 3. Agency e Intelligenza: una distinzione cruciale

Per costruire questo quadro, è necessario introdurre una distinzione fondamentale. Gran parte della confusione attorno all'IA deriva dal fatto che usiamo una sola parola, *intelligenza*, per designare realtà molto diverse.

Da un lato c'è la capacità di agire efficacemente: produrre risultati utili, risolvere compiti, raggiungere obiettivi. Dall'altro c'è la capacità di comprendere: attribuire senso, avere intenzioni, cogliere un contesto. L'IA generativa eccelle nella prima dimensione e non possiede la seconda. È proprio questa asimmetria a renderla al tempo stesso estremamente potente ed estremamente facile da malinterpretare.

*   **L’Intelligenza** (in senso forte) implica comprensione semantica, intenzionalità, coscienza.
*   **L’Agency** è la capacità di agire con successo per raggiungere un obiettivo.

Per capire questo disaccoppiamento, possiamo usare la metafora del **cavallo vapore (HP)**. Alla fine del XVIII secolo, James Watt usò la potenza dei cavalli da tiro per misurare l'efficacia delle sue macchine a vapore. Il termine "cavallo vapore" servì a vendere la macchina, ma oggi a nessuno verrebbe in mente di cercare zoccoli o una criniera all'interno di un motore a combustione. Allo stesso modo, dovremmo smettere di cercare proprietà cognitive o psicologiche all'interno di sistemi informazionali e computazionali. L'IA è pura potenza di calcolo (Agency), non una mente sintetica (Intelligenza).

Gli esseri umani combinano entrambe. I software tradizionali avevano un'Agency limitata e nessuna Intelligenza. L'IA generativa introduce qualcosa di radicalmente nuovo: un'**Agency elevatissima, senza Intelligenza**.

Un modello linguistico può scrivere codice o un saggio strutturato, ma non sa cosa sia un computer né cosa significhi ciò che produce. Manipola simboli, non comprensioni. È pura forza bruta. L'intelligenza che permette di raggiungere un obiettivo è quella dell'essere umano che pilota questa forza.

---

## 4. Perché tendiamo a fidarci troppo: il meccanismo del riconoscimento

Sorge allora una domanda naturale: se sappiamo, almeno razionalmente, che l'IA non capisce, perché continuiamo a comportarci come se fosse affidabile quanto una persona?

La risposta non è tecnica, ma cognitiva. Quando un sistema usa il linguaggio in modo fluido, il nostro cervello attiva automaticamente meccanismi di fiducia, empatia e attribuzione di intenzionalità. È lo stesso automatismo che usiamo nelle nostre relazioni umane quotidiane.

Quando parliamo con una persona, non ci chiediamo cosa abbiamo di fronte: la riconosciamo immediatamente come essere umano, dal suo modo di muoversi, di parlare, di comportarsi.

> Il nostro rapporto con gli altri non si basa sulla *conoscenza*, ma sul **riconoscimento**: un atteggiamento pratico, automatico.

Il linguaggio fluido dell'IA attiva questo stesso meccanismo. Non perché l'IA lo meriti, ma perché il nostro cervello distingue male tra un uso autentico del linguaggio e una simulazione convincente.

Questo meccanismo funziona benissimo tra umani, ma diventa pericoloso quando applicato a sistemi che non hanno né comprensione né responsabilità. Ne risulta uno slittamento sottile: smettiamo di verificare, accettiamo output plausibili come se fossero veri, e deleghiamo giudizi che dovrebbero rimanere umani.

---

## 5. Quando il riconoscimento fallisce

Il problema del riconoscimento mal riposto non è nuovo nella storia umana. Cambia forma, ma obbedisce a una logica ricorrente.

Nella storia, alcuni dei peggiori crimini sono stati commessi quando abbiamo smesso di riconoscere l'umanità di altri esseri umani, trattandoli come oggetti o bestie. Oggi, il rischio è inverso ma simmetrico: **attribuire qualità umane a ciò che è, in realtà, solo uno strumento.**

In entrambi i casi, il risultato è lo stesso: decisioni sbagliate, responsabilità mal distribuite, sistemi fragili. Nel mondo del lavoro, questo si traduce in una fiducia mal riposta, processi opachi e rischi difficili da gestire.

Questo parallelismo permette di chiarire un punto essenziale: trattare correttamente le persone e le tecnologie non è solo una questione etica, è una condizione dell'efficacia operativa.

---

## 6. L’IA come moltiplicatore

Una volta chiariti i rischi concettuali, è possibile ribaltare la prospettiva. Se l'IA non è un soggetto, ma uno strumento dotato di un'Agency estremamente elevata, come usarla intelligentemente?

La risposta consiste nel riconoscere che l'IA non livella le competenze: **le amplifica**. I timori di sostituzione si basano su una visione additiva (l'IA fa il mio lavoro). In realtà, il suo impatto è moltiplicativo.

Una fotocamera avanzata nelle mani di un amatore produce risultati mediocri; nelle mani di un grande fotografo, produce arte. Il valore non risiede dunque unicamente nella tecnologia, ma nella qualità delle competenze che la guidano.

Il **Prompt Engineering** non è una competenza tecnica isolata, ma una pratica che esige una competenza reale. Un prompt efficace nasce dall'intersezione di due competenze distinte:
1.  La competenza sintattica (adattarsi alla macchina).
2.  La competenza semantica di dominio (sapere cosa si sta chiedendo).

Se una delle due manca, il risultato si degrada rapidamente. Dobbiamo adattare sintatticamente i nostri prompt all'IA, perché è lei che non può adattarsi a noi (siamo noi quelli intelligenti, non lei). Ma dobbiamo anche sapere quali domande hanno senso, capire cosa manca in una risposta, verificarla e identificarne gli errori.

---

## 7. Contro l’omologazione: l’architetto e il muratore

L'IA lavora sulla media statistica e tende alla convergenza. Produce, ad esempio, testi corretti ma spesso piatti. Questo ci indica chiaramente cosa non può fare: **divergere in modo significativo.**

È proprio qui che il ruolo umano diventa non solo pertinente, ma indispensabile. La strategia, lo stile, la sensibilità al contesto e l'assunzione di responsabilità non possono essere delegati. Una soluzione nuova a un problema nuovo non può essere generata dall'IA.

La valore umano risiede nella **divergenza**: nello stile, nell'eccezione, nell'intuizione che spezza la regola quando è necessario.

Pensare il lavoro come una collaborazione strutturata tra persone e sistemi, con ruoli chiari e non sovrapposti, permette di evitare sia l'idolatria tecnologica che il rifiuto difensivo.

L'umano è innanzitutto l'**Architetto**: definisce la direzione, la strategia, i vincoli e le relazioni. Diventa poi l'**Arbitro**: valuta il lavoro, lo corregge e ne assume la responsabilità. Tra i due interviene questa Agency estremamente potente che, per errore, abbiamo deciso di chiamare intelligenza.

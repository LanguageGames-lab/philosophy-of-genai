# Agency, intelligence et l’erreur de reconnaissance à l’ère de l’IA

**Auteur :** [LanguageGames-lab]  
**Date :** Janvier 2025

---

## Introduction

Lorsqu’un système nous répond de manière fluide, articulée et cohérente, notre premier réflexe n’est pas technique : il est relationnel. Nous avons tendance à nous comporter comme s’il y avait, de l’autre côté, quelqu’un qui comprend ce qu’il dit.

Cette tendance à chercher un visage derrière chaque mot, une intention derrière chaque réponse, un esprit derrière tout comportement cohérent est un raccourci cognitif qui nous a permis de survivre en tant qu’espèce sociale. Mais aujourd’hui, à l’ère de l’intelligence artificielle générative, ce même mécanisme risque de devenir un piège : il nous conduit à confondre la qualité de l’exécution avec la compréhension, et la capacité à produire des réponses convaincantes avec une forme d’intelligence qui, en réalité, n’existe pas.

> *Ce n'est pas un bug dans l'IA. C'est un bug dans notre cerveau.*

Ce phénomène porte un nom précis forgé par Luciano Floridi : la **paréidolie sémantique**.

Tout comme la paréidolie visuelle nous fait voir des visages dans les nuages, la paréidolie sémantique nous pousse à projeter de l'intentionnalité, du sens et de la conscience là où il n'y a que de la statistique et de la reconnaissance de formes (*pattern matching*) à grande échelle. Nous percevons un esprit là où il n'existe qu'une extraordinaire capacité à manipuler des symboles.

[![Clicca per ingrandire](../images/infografica-agency-fr.png)](../images/infografica-agency-fr.png)
---

## 1. Le paradoxe d’Eliza : quand le code paraît humain

En 1966, au MIT, Joseph Weizenbaum développe un programme appelé **Eliza**. D’un point de vue technique, Eliza était extrêmement simple : quelques règles syntaxiques permettant de reformuler les phrases de l’utilisateur sous forme de questions, en imitant le style d’un psychothérapeute rogérien. Aucune compréhension, aucune mémoire profonde, aucune conscience.

Et pourtant, quelque chose de surprenant se produisit. Des étudiants et des collaborateurs commencèrent à parler avec Eliza comme s’il s’agissait d’une personne réelle. La secrétaire de Weizenbaum allait jusqu’à lui demander de quitter la pièce afin de pouvoir poursuivre la conversation « en privé ».

Le point crucial n’est pas que les personnes aient été trompées techniquement : elles savaient parfaitement qu’Eliza était un programme. Et pourtant, sur le plan émotionnel et cognitif, elles n’arrivaient pas à la traiter comme une simple machine.

Ce phénomène est entré dans l’histoire sous le nom d’**effet Eliza** : la tendance humaine à attribuer intentionnalité, compréhension et intériorité à des systèmes qui se contentent de simuler le langage. Aujourd’hui, avec des modèles comme ChatGPT, cet effet a été amplifié de manière considérable.

Le modèle ne comprend rien, ne pense à rien, ne ressent rien. Il manipule des symboles selon des patterns statistiques. C'est de la pure force brute computationnelle. Mais notre cerveau, lui, ne fait pas la différence.

Quand l'IA nous écrit un code propre, un email professionnel ou une analyse structurée, on active automatiquement des mécanismes de confiance interpersonnelle. On cesse de vérifier. Le vrai danger ce n’est pas que l'IA se trompe (elle se trompe tout le temps), mais que nous ne vérifions plus parce que ça sonne juste.

---

## 2. La quatrième révolution : l’expulsion de l’info-sphère

Pour comprendre pourquoi nous sommes si vulnérables à cette illusion, il faut élargir le champ de vision. On peut décrire notre époque comme le résultat de quatre grands chocs qui ont progressivement décentré l’être humain.

*   **Copernic** nous a retirés du centre de l’univers.
*   **Darwin** nous a retirés du centre du vivant.
*   **Freud** nous a retirés du centre de notre propre esprit.
*   **Turing** (aujourd'hui) nous retire du centre de l’**info-sphère**.

Pour la première fois dans l’histoire, nous ne sommes plus les seuls agents capables de manipuler l’information de manière efficace. Les algorithmes ne pensent pas comme nous, mais ils traitent des données à une vitesse et à une échelle qui dépassent largement les capacités humaines.

Cela nous expulse du centre de l’écosystème informationnel et nous oblige à cohabiter avec de nouveaux agents numériques. Nous n’adoptons pas simplement un nouveau logiciel : nous vivons une transformation profonde de la manière dont l’être humain se situe par rapport à l’information. Pendant des siècles, nous avons tenu pour acquis que nous étions les seuls sujets capables d’analyser, d’interpréter et de produire une connaissance complexe. Ce n’est plus le cas aujourd’hui.

> Nous avons adopté un outil d'une puissance extraordinaire, mais nous manquons encore de garde-fous épistémologiques, d’un cadre, pour distinguer systématiquement la vraisemblance syntaxique de l’expression sémantique.

---

## 3. Agency et intelligence : une distinction cruciale

Pour construire ce cadre, il est nécessaire d’introduire une distinction fondamentale. Une grande partie de la confusion autour de l’IA vient du fait que nous utilisons un seul mot, *intelligence*, pour désigner des réalités très différentes.

D’un côté, il y a la capacité d’agir efficacement : produire des résultats utiles, résoudre des tâches, atteindre des objectifs. De l’autre, il y a la capacité de comprendre : attribuer du sens, avoir des intentions, saisir un contexte. L’IA générative excelle dans la première dimension et ne possède pas la seconde. C’est précisément cette asymétrie qui la rend à la fois extrêmement puissante et extrêmement facile à mal interpréter.

*   **L’Intelligence** (au sens fort) implique une compréhension sémantique, une intentionnalité, une conscience.
*   **L’Agency** est la capacité d’agir avec succès pour atteindre un objectif.

Pour comprendre ce découplage, on peut utiliser la métaphore du **cheval-vapeur (HP)**. À la fin du XVIIIe siècle, James Watt a utilisé la puissance des chevaux de trait pour mesurer l'efficacité de ses machines à vapeur. Le terme "cheval-vapeur" a servi à vendre la machine, mais aujourd'hui, personne ne songerait à chercher des sabots ou une crinière à l'intérieur d'un moteur à combustion. De la même manière, nous devrions cesser de chercher des propriétés cognitives ou psychologiques au sein de systèmes informationnels et computationnels. L'IA est une pure puissance de calcul (Agency), pas un esprit synthétique (Intelligence).

Les êtres humains combinent les deux. Les logiciels traditionnels avaient une agency limitée et aucune intelligence. L’IA générative introduit quelque chose de radicalement nouveau : une **Agency très élevée, sans Intelligence**.

Un modèle linguistique peut écrire du code ou un essai structuré, mais il ne sait pas ce qu’est un ordinateur ni ce que signifie ce qu’il produit. Il manipule des symboles, pas des compréhensions. C’est de la pure force brute. L’intelligence qui permet d’atteindre un objectif est celle de l’être humain qui pilote cette force.

---

## 4. Pourquoi nous avons tendance à trop lui faire confiance : le mécanisme de la reconnaissance

Une question se pose alors naturellement : si nous savons, au moins rationnellement, que l’IA ne comprend pas, pourquoi continuons-nous à nous comporter comme si elle était aussi fiable qu’une personne ?

La réponse n’est pas technique, mais cognitive. Lorsqu’un système utilise le langage de manière fluide, notre cerveau active automatiquement des mécanismes de confiance, d’empathie et d’attribution d’intentionnalité. C’est le même automatisme que nous utilisons dans nos relations humaines quotidiennes.

Lorsque nous parlons à une personne, nous ne nous demandons pas ce que nous avons en face de nous : nous le reconnaissons immédiatement comme un être humain, à partir de sa manière de se mouvoir, de parler, de se comporter.

> Notre rapport aux autres ne repose pas sur la *connaissance*, mais sur la **reconnaissance** : une attitude pratique, automatique.

Le langage fluide de l’IA active ce même mécanisme. Non pas parce que l’IA le mérite, mais parce que notre cerveau distingue mal entre un usage authentique du langage et une simulation convaincante.

Ce mécanisme fonctionne très bien entre humains, mais devient dangereux lorsqu’il est appliqué à des systèmes qui n’ont ni compréhension ni responsabilité. Il en résulte un glissement subtil : nous cessons de vérifier, nous acceptons des outputs plausibles comme s’ils étaient vrais, et nous déléguons des jugements qui devraient rester humains.

---

## 5. Quand la reconnaissance échoue

Le problème de la reconnaissance mal placée n’est pas nouveau dans l’histoire humaine. Il change de forme, mais obéit à une logique récurrente.

Dans l’histoire, certains des pires crimes ont été commis lorsque nous avons cessé de reconnaître l’humanité d’autres êtres humains, en les traitant comme des objets ou des bêtes. Aujourd’hui, le risque est inverse mais symétrique : **attribuer des qualités humaines à ce qui n’est, en réalité, qu’un outil.**

Dans les deux cas, le résultat est le même : mauvaises décisions, responsabilités mal réparties, systèmes fragiles. Dans le monde du travail, cela se traduit par une confiance mal placée, des processus opaques et des risques difficiles à maîtriser.

Ce parallélisme permet de clarifier un point essentiel : traiter correctement les personnes et les technologies n’est pas seulement une question éthique, c’est une condition de l’efficacité opérationnelle.

---

## 6. L’IA comme multiplicateur

Une fois les risques conceptuels clarifiés, il est possible de renverser la perspective. Si l’IA n’est pas un sujet, mais un outil doté d’une agency extrêmement élevée, comment l’utiliser intelligemment ?

La réponse consiste à reconnaître que l’IA ne nivelle pas les compétences : **elle les amplifie**. Les craintes de substitution reposent sur une vision additive (l’IA fait mon travail). En réalité, son impact est multiplicatif.

Un appareil photo avancé entre les mains d’un amateur produit des résultats médiocres ; entre les mains d’un grand photographe, il produit de l’art. La valeur ne réside donc pas uniquement dans la technologie, mais dans la qualité des compétences qui la guident.

Le **Prompt Engineering** n’est pas une compétence technique isolée, mais une pratique qui exige une compétence réelle. Un prompt efficace naît de l’intersection de deux compétences distinctes :
1.  La compétence syntaxique (s’adapter à la machine).
2.  La compétence sémantique de domaine (savoir ce que l’on demande).

Si l’une des deux fait défaut, le résultat se dégrade rapidement. Nous devons adapter syntaxiquement nos prompts à l’IA, car c’est elle qui ne peut pas s’adapter à nous (c’est nous qui sommes intelligents, pas elle). Mais nous devons aussi savoir quelles questions ont du sens, comprendre ce qui manque dans une réponse, la vérifier et en identifier les erreurs.

---

## 7. Contre l’uniformisation : l’architecte et le maçon

L’IA travaille sur la moyenne statistique et tend vers la convergence. Elle produit, par exemple, des textes corrects mais souvent plats. Cela nous indique clairement ce qu’elle ne peut pas faire : **diverger de manière significative.**

C’est précisément là que le rôle humain devient non seulement pertinent, mais indispensable. La stratégie, le style, la sensibilité au contexte et l’assomption de responsabilité ne peuvent pas être délégués. Une solution nouvelle à un problème nouveau ne peut pas être générée par l’IA.

La valeur humaine réside dans la **divergence** : dans le style, dans l’exception, dans l’intuition qui brise la règle lorsque c’est nécessaire.

Penser le travail comme une collaboration structurée entre des personnes et des systèmes, avec des rôles clairs et non superposés, permet d’éviter à la fois l’idolâtrie technologique et le rejet défensif.

L’humain est d’abord l’**Architecte** : il définit la direction, la stratégie, les contraintes et les relations. Il devient ensuite l’**Arbitre** : il évalue le travail, le corrige et en assume la responsabilité. Entre les deux intervient cette Agency extrêmement puissante que, par erreur, nous avons décidé d’appeler intelligence.
